{
  "cells": [
    {
      "metadata": {
        "id": "df840597_"
      },
      "cell_type": "markdown",
      "source": [
        "\u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/google-ai-edge/LiteRT/blob/main/litert/samples/colab/LiteRT_AOT_Compilation_Tutorial.ipynb\"\u003e\n",
        "  \u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\n",
        "\u003c/a\u003e"
      ]
    },
    {
      "metadata": {
        "id": "qlBoQghUVJ9_"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2025 The AI Edge Authors."
      ]
    },
    {
      "metadata": {
        "cellView": "form",
        "id": "hdBh2ypwVFJ6"
      },
      "cell_type": "code",
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fkJOsPQjVJ_u"
      },
      "cell_type": "markdown",
      "source": [
        "# LiteRT NPU AOT Compilation, and Play for On-device AI integration"
      ]
    },
    {
      "metadata": {
        "id": "fxPugexEVwGt"
      },
      "cell_type": "markdown",
      "source": [
        "In this colab, you'll learn how to use the LiteRT AOT (ahead of time) Compiler to compile an Selfie Segmentation model from either PyTorch or TFLite model, to LiteRT models that are optimized and compiled for on-device NPUs.\n",
        "\n",
        "The models we are using are originally published on Mediapipe [Image segmentation guide](https://ai.google.dev/edge/mediapipe/solutions/vision/image_segmenter). The two models used in this colabs are\n",
        "* Selfie Segmentation: A model that segment the portrait of a person, and can be used for replacing or modifying the background in an image.\n",
        "* Selfie Multiclass: A LiteRT model that takes an image of a person, locates areas for different areas such as hair, skin, and clothing, and outputs an image segmentation map for these items.\n",
        "\n",
        "This colab will also walk you through the steps for preparing the models with **[Play for On-device AI]( https://developer.android.com/google/play/on-device-ai) (PODAI)**.\n",
        "\n",
        "**PODAI** deliver custom models for on-device AI features more efficiently with Play for On-device AI. Google Play simplifies launching, targeting, versioning, downloading. Together with LiteRT NPU AOT Compilation, developers can deliver compiled ML models for variant end devices without the need to know what NPUs end users' phone is equiped with."
      ]
    },
    {
      "metadata": {
        "id": "RWRjeazEie0h"
      },
      "cell_type": "markdown",
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "metadata": {
        "id": "yX2LGZG1il9M"
      },
      "cell_type": "markdown",
      "source": [
        "### Install the required packages\n",
        "Start by installing the required packages, including the ai-edge-litert which contains the NPU AOT compiler, and other libraries you'll use for model conversion."
      ]
    },
    {
      "metadata": {
        "id": "zQIk9wdfT4j3"
      },
      "cell_type": "code",
      "source": [
        "# Install libc++ dependencies\n",
        "\n",
        "!wget https://apt.llvm.org/llvm.sh\n",
        "!chmod +x llvm.sh\n",
        "!./llvm.sh 18 all\n",
        "!apt-get install -y libc++-18-dev libc++abi-18-dev"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "1mP8rzyxk00T"
      },
      "cell_type": "code",
      "source": [
        "!pip install ai-edge-litert-sdk-mediatek-nightly\n",
        "# Takes ~5 minutes to download and build the package\n",
        "!pip install ai-edge-litert-sdk-qualcomm-nightly\n",
        "!pip install ai-edge-litert-nightly\n",
        "!pip install ai-edge-torch"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "jnF9XRfhk_KH"
      },
      "cell_type": "markdown",
      "source": [
        "Import the required packages."
      ]
    },
    {
      "metadata": {
        "id": "3jb-_lOnk9Ob"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "from ai_edge_litert.aot import aot_compile as aot_lib\n",
        "from ai_edge_litert.aot.ai_pack import export_lib as ai_pack_export\n",
        "from ai_edge_litert.aot.vendors.mediatek import target as mtk_target\n",
        "from ai_edge_litert.aot.vendors.qualcomm import target as qnn_target\n",
        "import ai_edge_torch\n",
        "from ai_edge_torch.examples.selfie_segmentation import model as selfie_segmentation_model_lib\n",
        "import huggingface_hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Gc3CiD4Y8qUq"
      },
      "cell_type": "markdown",
      "source": [
        "## Quickstart"
      ]
    },
    {
      "metadata": {
        "id": "6jOk5uUoqmCn"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare the PyTorch Model\n",
        "\n",
        "Here let's use the [MediaPipe Selfie Segmentation](https://storage.googleapis.com/mediapipe-assets/Model%20Card%20MediaPipe%20Selfie%20Segmentation.pdf) model as the starting point.\n",
        "\n",
        "This model is ported to PyTorch. The Torch implementation is available under [ai_edge_torch/examples/selfie_segmentation](https://github.com/google-ai-edge/ai-edge-torch/blob/main/ai_edge_torch/examples/selfie_segmentation/model.py).\n",
        "\n",
        "The weights of the model is available at HuggingFace [litert-community/MediaPipe-Selfie-Segmentation](https://huggingface.co/litert-community/MediaPipe-Selfie-Segmentation)."
      ]
    },
    {
      "metadata": {
        "id": "W2l5buNTn-WU"
      },
      "cell_type": "code",
      "source": [
        "selfie_segmentation = selfie_segmentation_model_lib.SelfieSegmentation()\n",
        "\n",
        "# Download the weights from Hugging Face Hub\n",
        "work_dir = \"selfie_segmentation\"\n",
        "os.makedirs(work_dir, exist_ok=True)\n",
        "\n",
        "weights_path = huggingface_hub.hf_hub_download(\n",
        "    repo_id=\"litert-community/MediaPipe-Selfie-Segmentation\",\n",
        "    filename=\"selfie_segmentation.pth\",\n",
        ")\n",
        "selfie_segmentation.load_state_dict(torch.load(weights_path))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "pHj42h58xM9J"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert to LiteRT model, with NPU AOT compilation.\n",
        "\n",
        "Let's first follow regular PyTorch to LiteRT conversion using ai_edge_torch. The `convert` function provided by the ai_edge_torch package allows conversion from a PyTorch model to an on-device model.\n",
        "\n",
        "In order to support compiling to NPUs, we just need to add additional method `experimental_add_compilation_backend`. By default (if you don't provide any target backends) the model will be converted and compiled to all registered backends. For now the backend includes:\n",
        "* General model runnable on LiteRT CPU and GPU.\n",
        "* Qualcomm NPUs.\n",
        "* MediaTek NPUs."
      ]
    },
    {
      "metadata": {
        "id": "YLwVDAZdxMd9"
      },
      "cell_type": "code",
      "source": [
        "# Converts model I/O to Channel Last layout\n",
        "channel_last_selfie_segmentation = ai_edge_torch.to_channel_last_io(\n",
        "    selfie_segmentation, args=[0], outputs=[0]\n",
        ")\n",
        "sample_input = (torch.randn(1, 256, 256, 3),)\n",
        "compiled_models = ai_edge_torch.experimental_add_compilation_backend().convert(\n",
        "    channel_last_selfie_segmentation.eval(), sample_input\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "EjEg6x9-7U_6"
      },
      "cell_type": "markdown",
      "source": [
        "From the logs, we can see some backends failed for compilation. This is because for older generations of NPUs, executing floating point model is generally not supported. Failing backends will be skipped gracefully for batch compilation.\n",
        "\n",
        "We can also inspect the compilation status using `compilation_report` API.\n",
        "\n",
        "In addition to the compilation status, the report also contains the following information:\n",
        "* For each backend, if compilation succeeded, how much of the graph is offloaded to NPU, or the full model is compiled.\n",
        "* Error log for failed backends."
      ]
    },
    {
      "metadata": {
        "id": "XRjaSzZ27Y-h"
      },
      "cell_type": "code",
      "source": [
        "# @title Print Compilation Report\n",
        "\n",
        "print(compiled_models.compilation_report())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "JqB9bCOv9DIg"
      },
      "cell_type": "markdown",
      "source": [
        "### Export and Validation on CPU.\n",
        "\n",
        "Once the compilation finishes, we can use `model.export` method to export all models to disk.\n",
        "\n",
        "By default, the models will be stored in a flat structure in output directory, with each model name suffixed with the backend id.\n",
        "\n",
        "For example:\n",
        "\n",
        "| Model File Name                            | Backend  | SoC    | Note               |\n",
        "|--------------------------------------------|----------|--------|--------------------|\n",
        "| selfie_segmentation_fallback.tflite        | CPU/GPU  | N/A    | N/ A               |\n",
        "| selfie_segmentation_Qualcomm_SM8450.tflite | Qualcomm | SM8450 | Snapdragon 8 Gen 1 |\n",
        "| selfie_segmentation_MediaTek_mt6989.tflite | MediaTek | mt6989 | Dimensity 9300     |"
      ]
    },
    {
      "metadata": {
        "id": "e_C0uNHR58fh"
      },
      "cell_type": "code",
      "source": [
        "# Saving models to disk.\n",
        "\n",
        "compiled_models.export(work_dir, model_name='selfie_segmentation')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "LMvN9HPntj41"
      },
      "cell_type": "code",
      "source": [
        "# Downloading Testing image\n",
        "\n",
        "test_image = huggingface_hub.hf_hub_download(\n",
        "    repo_id=\"litert-community/MediaPipe-Selfie-Segmentation\",\n",
        "    filename=\"test_img.png\",\n",
        ")\n",
        "pil_image = Image.open(test_image).convert(\"RGB\").resize((256, 256))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "nSHmFQ_7NIni"
      },
      "cell_type": "code",
      "source": [
        "# Run PyTorch with test image\n",
        "\n",
        "channel_first_numpy_array = np.array(pil_image, dtype=np.float32)[\n",
        "    None, ...\n",
        "].transpose(0, 3, 1, 2)\n",
        "torch_mask_out = (\n",
        "    selfie_segmentation(torch.from_numpy(channel_first_numpy_array))\n",
        "    .detach()\n",
        "    .numpy()\n",
        "    .transpose(0, 2, 3, 1)\n",
        ")\n",
        "torch_uint8_mask = (torch_mask_out.reshape((256, 256)) * 255).astype(np.uint8)\n",
        "torch_mask_image = Image.fromarray(torch_uint8_mask, mode='L')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "q0CSBX38wJhv"
      },
      "cell_type": "code",
      "source": [
        "# Run LiteRT with test image\n",
        "from ai_edge_litert.compiled_model import CompiledModel\n",
        "\n",
        "numpy_array = np.array(pil_image, dtype=np.float32)[None, ...]\n",
        "cpu_model_path = os.path.join(work_dir, \"selfie_segmentation_fallback.tflite\")\n",
        "cm_model = CompiledModel.from_file(cpu_model_path)\n",
        "sig_idx = 0\n",
        "input_buffers = cm_model.create_input_buffers(sig_idx)\n",
        "output_buffers = cm_model.create_output_buffers(sig_idx)\n",
        "input_buffers[0].write(numpy_array)\n",
        "cm_model.run_by_index(sig_idx, input_buffers, output_buffers)\n",
        "uint8_mask = (\n",
        "    output_buffers[0].read(256 * 256, np.float32).reshape((256, 256)) * 255\n",
        ").astype(np.uint8)\n",
        "mask_image = Image.fromarray(uint8_mask, mode=\"L\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "PG1mEDsY60_p"
      },
      "cell_type": "code",
      "source": [
        "# Show output results\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
        "\n",
        "for idx, (title, image) in enumerate([\n",
        "    ('Test Image', pil_image),\n",
        "    ('PyTorch Mask Image', torch_mask_image),\n",
        "    ('TFLite Mask Image', mask_image),\n",
        "]):\n",
        "  axes[idx].imshow(image)\n",
        "  axes[idx].set_title(title)\n",
        "  axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3k5ejjeFAf-D"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Exporting Models for Google Play On-Device AI (PODAI)\n",
        "\n",
        "With your models verified, the next essential step is preparing them for deployment. This section details how to package your compiled models for upload to Google Play, enabling delivery to user devices through the On-Demand AI (PODAI) framework.\n",
        "\n",
        "The AiEdgeLiteRT AOT (Ahead-of-Time) module provides `ai_pack` utilities specifically for this purpose. These utilities create an **AI Pack**, which is a crucial data asset. An AI Pack bundles your compiled models with device-targeting configurations, ensuring that the correct models and assets are delivered to the appropriate user devices. This is particularly vital for NPU (Neural Processing Unit) compilations, as it guarantees that models optimized for a specific System-on-Chip (SoC) reach only the devices equipped with that SoC."
      ]
    },
    {
      "metadata": {
        "id": "sYU2eo1EC_M9"
      },
      "cell_type": "code",
      "source": [
        "# Configuring the AI Pack\n",
        "ai_pack_dir = os.path.join(work_dir, 'ai_pack')\n",
        "ai_pack_name = 'selfie_segmentation'\n",
        "litert_model_name = 'segmentation_model'\n",
        "\n",
        "# Clean up\n",
        "shutil.rmtree(ai_pack_dir, ignore_errors=True)\n",
        "\n",
        "# Export\n",
        "ai_pack_export.export(\n",
        "    compiled_models, ai_pack_dir, ai_pack_name, litert_model_name\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "SPhOkAzZFCi8"
      },
      "cell_type": "markdown",
      "source": [
        "And now the models are ready for comsuption by PODAI!\n",
        "\n",
        "Now we will move on to Android Studio for the following steps, please refer to [THIS LINK](https://github.com/google-ai-edge/LiteRT/tree/main/litert/samples/image_segmentation/kotlin_npu) for details.\n",
        "\n",
        "But if you are curious on the contents of AI Pack, we can take a look into the directory."
      ]
    },
    {
      "metadata": {
        "id": "rLe3lqwPEtw-"
      },
      "cell_type": "code",
      "source": [
        "# @title Inspecting AI Pack source\n",
        "\n",
        "\n",
        "def list_files(startpath):\n",
        "  \"\"\"Function to print out the tree structure of a directory.\"\"\"\n",
        "  for root, dirs, files in os.walk(startpath):\n",
        "    level = root.replace(startpath, '').count(os.sep)\n",
        "    indent = ' ' * 4 * (level)\n",
        "    print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "    subindent = ' ' * 4 * (level + 1)\n",
        "    for f in files:\n",
        "      print('{}{}'.format(subindent, f))\n",
        "\n",
        "\n",
        "list_files(ai_pack_dir)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "8FQFMlUoF3ZZ"
      },
      "cell_type": "markdown",
      "source": [
        "## Advanced Usage\n",
        "\n",
        "This section covers advanced usages like compiling a LiteRT (TFLite) model directly.\n"
      ]
    },
    {
      "metadata": {
        "id": "feD3Gil6Hwl4"
      },
      "cell_type": "markdown",
      "source": [
        "### NPU Compilation from TFLite model\n",
        "\n",
        "In many of the cases, you might already have a TFLite converted models, which was published before, but the source model is not yet available or the source model is not a PyTorch model. In this case, instead of AiEdgeTorch package, you can use the APIs provided by AiEdgeLiteRT compiler directly."
      ]
    },
    {
      "metadata": {
        "id": "3UZmsqYkIGpJ"
      },
      "cell_type": "markdown",
      "source": [
        "#### Getting the TFLite Model\n",
        "\n",
        "We will use [MediaPipe MultiClass Segmentation](https://storage.googleapis.com/mediapipe-assets/Model%20Card%20Multiclass%20Segmentation.pdf) model for this use case.\n",
        "\n",
        "The TFLite model is available from [MediaPipe Image segmentation](https://ai.google.dev/edge/mediapipe/solutions/vision/image_segmenter#multiclass-model) page.\n"
      ]
    },
    {
      "metadata": {
        "id": "I_MF7QfsICSL"
      },
      "cell_type": "code",
      "source": [
        "work_dir = '.'\n",
        "\n",
        "model_url = 'https://storage.googleapis.com/mediapipe-models/image_segmenter/selfie_multiclass_256x256/float32/latest/selfie_multiclass_256x256.tflite'\n",
        "tflite_model_path = os.path.join(work_dir, 'selfie_multiclass_256x256.tflite')\n",
        "\n",
        "model_content = requests.get(model_url)\n",
        "\n",
        "with open(tflite_model_path, 'wb') as fout:\n",
        "  fout.write(model_content.content)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "1D3ZARwbT8cI"
      },
      "cell_type": "markdown",
      "source": [
        "#### Using LiteRT Python API to quickly verify the TfLite model\n",
        "\n",
        "In the following example, we will show both mask image and blended result."
      ]
    },
    {
      "metadata": {
        "id": "OsxNMD2tB6tY"
      },
      "cell_type": "code",
      "source": [
        "from ai_edge_litert.compiled_model import CompiledModel\n",
        "\n",
        "SEGMENT_COLORS = [\n",
        "    (0, 0, 0),\n",
        "    (255, 0, 0),\n",
        "    (0, 255, 0),\n",
        "    (0, 0, 255),\n",
        "    (255, 255, 0),\n",
        "    (255, 0, 255),\n",
        "]\n",
        "INPUT_SIZE = (256, 256)\n",
        "NUM_CLASSES = 6\n",
        "\n",
        "# Load the model and image\n",
        "model = CompiledModel.from_file(tflite_model_path)\n",
        "original_image = np.array(Image.open(test_image).convert('RGB'))\n",
        "img_array = np.array(pil_image).astype(np.float32)\n",
        "\n",
        "# Normalize the image\n",
        "normalized = (img_array - 127.5) / 127.5\n",
        "normalized = np.ascontiguousarray(normalized, dtype=np.float32)\n",
        "\n",
        "# Run inference\n",
        "sig_idx = 0\n",
        "input_buffers = model.create_input_buffers(sig_idx)\n",
        "output_buffers = model.create_output_buffers(sig_idx)\n",
        "input_data = normalized.reshape(-1)\n",
        "input_buffers[0].write(input_data)\n",
        "model.run_by_index(sig_idx, input_buffers, output_buffers)\n",
        "\n",
        "# Get output data\n",
        "height, width = INPUT_SIZE\n",
        "output_size = height * width * NUM_CLASSES\n",
        "output_data = output_buffers[0].read(output_size, np.float32)\n",
        "output_data = output_data.reshape(height, width, NUM_CLASSES)\n",
        "mask = np.argmax(output_data, axis=2).astype(np.uint8)\n",
        "\n",
        "# Create colored mask\n",
        "colored_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "for label_idx in range(NUM_CLASSES):\n",
        "  class_mask = mask == label_idx\n",
        "  color = SEGMENT_COLORS[label_idx]\n",
        "  colored_mask[class_mask] = color\n",
        "\n",
        "# Blend with original image\n",
        "# Resize colored mask to match original image if necessary\n",
        "if original_image.shape[:2] != colored_mask.shape[:2]:\n",
        "  colored_mask_pil = Image.fromarray(colored_mask)\n",
        "  colored_mask_pil = colored_mask_pil.resize(\n",
        "      (original_image.shape[1], original_image.shape[0])\n",
        "  )\n",
        "  colored_mask = np.array(colored_mask_pil)\n",
        "\n",
        "# Blend images with alpha 0.5\n",
        "alpha = 0.5\n",
        "blended_image = (original_image * (1 - alpha) + colored_mask * alpha).astype(\n",
        "    np.uint8\n",
        ")\n",
        "\n",
        "# Display them\n",
        "fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
        "\n",
        "for idx, (title, image) in enumerate([\n",
        "    ('Original Image', original_image),\n",
        "    ('Colored Mask', colored_mask),\n",
        "    ('Blended Image', blended_image),\n",
        "]):\n",
        "  axes[idx].imshow(image)\n",
        "  axes[idx].set_title(title)\n",
        "  axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "vk7UgqJbJbWp"
      },
      "cell_type": "markdown",
      "source": [
        "#### Convert to LiteRT model, with NPU AOT compilation.\n",
        "\n",
        "Since it's a TFLite model, we will use `ai_edge_litert.aot` module instead of AiEdgeTorch APIs."
      ]
    },
    {
      "metadata": {
        "id": "ZTxogg45JXzN"
      },
      "cell_type": "code",
      "source": [
        "compiled_models = aot_lib.aot_compile(tflite_model_path, keep_going=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "gQQWKoB7KZoE"
      },
      "cell_type": "markdown",
      "source": [
        "For the following steps, it's the same as models compiled from PyTorch. e.g. For exporting for AI Pack"
      ]
    },
    {
      "metadata": {
        "id": "wxEexM8AjJqx"
      },
      "cell_type": "code",
      "source": [
        "# Configuring the AI Pack\n",
        "os.makedirs('selfie_multiclass')\n",
        "ai_pack_dir = os.path.join('selfie_multiclass', 'ai_pack')\n",
        "ai_pack_name = 'selfie_multiclass'\n",
        "litert_model_name = 'segmentation_multiclass'\n",
        "\n",
        "# Clean up\n",
        "shutil.rmtree(ai_pack_dir, ignore_errors=True)\n",
        "\n",
        "# Export\n",
        "ai_pack_export.export(\n",
        "    compiled_models, ai_pack_dir, ai_pack_name, litert_model_name\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "JqL6mwRhjj50"
      },
      "cell_type": "code",
      "source": [
        "list_files(ai_pack_dir)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "5_FwgZKBKprO"
      },
      "cell_type": "markdown",
      "source": [
        "### NPU Compilation for specific device / NPU\n",
        "\n",
        "By default, LiteRT AOT compilation compiles to all registered backends. But for local developement, you might only want to compile for specific devices, say the development phones by hand. This is achievable by providing the compilation targets explicitly.\n",
        "\n",
        "The following example will compile to QualComm SM8450 SoC and MediaTek MT6989 SoC."
      ]
    },
    {
      "metadata": {
        "id": "wBPQUmx0LoiM"
      },
      "cell_type": "code",
      "source": [
        "# @title Specifying the compilation target\n",
        "\n",
        "sm8450_target = qnn_target.Target(qnn_target.SocModel.SM8450)\n",
        "mt6989_target = mtk_target.Target(mtk_target.SocModel.MT6989)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "HIhvEJ5MMT8I"
      },
      "cell_type": "code",
      "source": [
        "# @title Compiling from PyTorch model\n",
        "\n",
        "compiled_models = (\n",
        "    ai_edge_torch.experimental_add_compilation_backend(sm8450_target)\n",
        "    .experimental_add_compilation_backend(mt6989_target)\n",
        "    .convert(channel_last_selfie_segmentation, sample_input)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "G7v8tmxDM44a"
      },
      "cell_type": "code",
      "source": [
        "# @title Compiling from TFLite model\n",
        "\n",
        "compiled_models = aot_lib.aot_compile(\n",
        "    tflite_model_path,\n",
        "    target=[sm8450_target, mt6989_target],\n",
        "    keep_going=False,  # We want to error out when there's failure.\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "CHMCCRYgNaI0"
      },
      "cell_type": "markdown",
      "source": [
        "# Read more\n",
        "\n",
        "More links goes here\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "provenance": [
        {
          "file_id": "1oPVuMnlM0aVPAS00rJ4qdWc_YIsBwGPh",
          "timestamp": 1747622569970
        },
        {
          "file_id": "//litert/samples/colab/LiteRT_AOT_Compilation_Tutorial.ipynb",
          "timestamp": 1747419284113
        },
        {
          "file_id": "1Stkm3eI4K_hdv4Lbcaw1KjKuWpctYcb3",
          "timestamp": 1746737834409
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
