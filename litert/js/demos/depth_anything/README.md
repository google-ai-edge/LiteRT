# LiteRT.js Depth Anything Demo

This demo uses LiteRT.js and the [Depth Anything V2 Small model](https://huggingface.co/docs/transformers/en/model_doc/depth_anything_v2) to generate a depth map from a single image in the browser.

## How to Run

1.  **Convert the Model:**

    This demo requires the model to be in TFLite format. Please follow the instructions in the [Converting the Model](#converting-the-model) section below.

2.  **Build the monorepo:**

    See instructions in the [LiteRT.js readme](https://github.com/google-ai-edge/LiteRT/blob/main/litert/js/README.md).

3.  **Start the Development Server:**

    ```bash
    npm run dev
    ```

    This will start a local development server and provide a URL to access the
    application.

4.  **Build for Deployment:**

    ```bash
    npm run build
    ```

    This will create a `dist` directory with the bundled application.

## Converting the Model

The Depth Anything model needs to be converted from its original format to TFLite to run with LiteRT.js. This can be done using [ai-edge-torch](https://github.com/google-ai-edge/ai-edge-torch) and optionally quantized using [ai-edge-quantizer](https://github.com/google-ai-edge/ai-edge-quantizer).

1.  **Set up a Python Virtual Environment:**

    It's recommended to use a virtual environment to avoid dependency conflicts.

    ```bash
    python -m venv .venv
    source .venv/bin/activate  # On Windows use `.venv\Scripts\activate`
    ```

2.  **Install Dependencies:**

    ```bash
    pip install ai-edge-torch ai-edge-quantizer transformers huggingface-hub torch pillow requests
    ```

3.  **Run the Conversion Script:**

    Run the `convert_model.py` script located in this directory:

    ```bash
    python convert_model.py
    ```
    This will download the model from Hugging Face, convert it to a float TFLite
    model, and then create a quantized version, saving both to the `static/`
    directory.

## How It Works

After you select an image, the demo:

1.  Resizes the image to the model's expected input dimensions.
2.  Preprocesses and converts the image data to the format required by the
    model (`[1, C, H, W]`).
3.  Runs Depth Anything v2 on LiteRT.js, which predicts the depth of the input
    tensor.
4.  Post-processes, normalizes, and colorizes this output
    (using a selected colormap) to create a visual representation of the depth.

## Models

The following models are used in this demo:

*   [**Depth Anything V2 Small**](https://huggingface.co/depth-anything/Depth-Anything-V2-Small-hf): A model for monocular depth estimation. The TFLite versions are generated by the conversion script above.

## File Structure

*   `index.html`: The main HTML file that loads the application.
*   `src/index.ts`: The main Lit component (`<depth-anything>`) that defines the UI and handles user interactions.
*   `src/depth_estimator.ts`: The core logic for running the depth estimation model, including preprocessing and post-processing.
*   `src/styles.ts`: The CSS styles for the Lit component.
*   `static/`: Contains the TFLite model files once they're converted.
*   `convert_model.py`: Script to convert and quantize the model.

